{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert Audio files to embeddings with VGGish network(CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/harritaylor/torchvggish.git\n",
    "%cd torchvggish\n",
    "!pip install librosa\n",
    "!pip install soundfile\n",
    "!pip install resampy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Drive helper and mount\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_src = '/content/drive/MyDrive/mostafavi/embeddings.pkl'\n",
    "audio_folder_path = '/content/drive/MyDrive/mostafavi/record'\n",
    "scores_filepath = '/content/drive/MyDrive/mostafavi/dataset.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start to extract embeddings with VGGish & save embeddings to a .pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "import tempfile\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def preprocess_audio(audio_path):\n",
    "    with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_wav_file:\n",
    "            wav_path = temp_wav_file.name  # Get the temporary file path\n",
    "            y, sr = sf.read(audio_path)  # Read MP3 using soundfile\n",
    "            sf.write(wav_path, y, sr, format='wav')  # Write to temporary WAV file\n",
    "\n",
    "    return wav_path\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "model = torch.hub.load('harritaylor/torchvggish', 'vggish')\n",
    "model.eval()\n",
    "def extract_embeddings(audio_path):\n",
    "    wav_path = preprocess_audio(audio_path)\n",
    "    print(\"#\",wav_path)\n",
    "    embeddings = model.forward(wav_path)\n",
    "    print(embeddings.shape)\n",
    "    return embeddings.detach().numpy()\n",
    "\n",
    "import os,re\n",
    "\n",
    "embeddings = []\n",
    "audio_filenames = os.listdir(audio_folder_path)\n",
    "for audio_path in sorted(audio_filenames, key=lambda filename: int(re.findall(r\"\\d+\", filename)[0])):\n",
    "    filepath = os.path.join(audio_folder_path, audio_path)\n",
    "    embedding = extract_embeddings(filepath)\n",
    "    embeddings.append(embedding)\n",
    "\n",
    "\n",
    "\n",
    "# Replace 'embeddings.pkl' with name that you want\n",
    "with open('embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load saved embeddigns and extract MFCCs from this with librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(embeddings_src, 'rb') as f:\n",
    "    loaded_embeddings = pickle.load(f)\n",
    "\n",
    "len(loaded_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define a function to convert VGGish embeddings to MFCCs\n",
    "def vggish_to_mfccs(vggish_embeddings):\n",
    "    # Assuming each VGGish embedding is a numpy array\n",
    "    mfccs = []\n",
    "    for embedding in vggish_embeddings:\n",
    "        # Transpose the embedding to match librosa's format (shape: (time, features))\n",
    "        embedding = embedding.T\n",
    "        \n",
    "        # Scale the embedding to have zero mean and unit variance\n",
    "        scaler = StandardScaler()\n",
    "        embedding_scaled = scaler.fit_transform(embedding)\n",
    "        \n",
    "        # Compute MFCCs from the scaled embedding\n",
    "        mfcc = librosa.feature.mfcc(S=embedding_scaled, sr=44100, n_mfcc=26)  # Adjust sr and n_mfcc as needed\n",
    "        mfccs.append(mfcc)\n",
    "    \n",
    "    return mfccs\n",
    "\n",
    "# Convert VGGish embeddings to MFCCs\n",
    "mfccs = vggish_to_mfccs(loaded_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because number of tracks is 580 but number of dataset is 299, we set both same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "scores_df = pd.read_csv(scores_filepath)\n",
    "scores = scores_df['ExaminationScore'].tolist()\n",
    "print(len(scores))\n",
    "mfccs = mfccs[:299]\n",
    "print(len(mfccs))\n",
    "embeddings_tensors = [torch.tensor(embedding) for embedding in mfccs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to Normalize embbeding tensors and pad them to have same size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def min_max_normalization(embeddings):\n",
    "    # Find the maximum sequence length and embedding dimension\n",
    "    max_seq_length = max(embedding.size(0) for embedding in embeddings)\n",
    "    max_embedding_dim = max(embedding.size(1) for embedding in embeddings)\n",
    "\n",
    "    # Add a batch dimension and reshape embeddings\n",
    "    reshaped_embeddings = [embedding.unsqueeze(0).unsqueeze(0) for embedding in embeddings]\n",
    "\n",
    "    # Resize embeddings to have the same sequence length and embedding dimension\n",
    "    resized_embeddings = [\n",
    "        torch.nn.functional.interpolate(\n",
    "            embedding,\n",
    "            size=(max_seq_length, max_embedding_dim),\n",
    "            mode='nearest'\n",
    "        ).squeeze(0).squeeze(0)\n",
    "        for embedding in reshaped_embeddings\n",
    "    ]\n",
    "\n",
    "    # Concatenate all embeddings along the batch dimension\n",
    "    concatenated_embeddings = torch.cat(resized_embeddings, dim=0)\n",
    "\n",
    "    # Find the minimum and maximum values in the concatenated embeddings\n",
    "    min_value = torch.min(concatenated_embeddings)\n",
    "    max_value = torch.max(concatenated_embeddings)\n",
    "\n",
    "    # Scale each embedding tensor individually\n",
    "    normalized_embeddings = [(embedding - min_value) / (max_value - min_value) for embedding in resized_embeddings]\n",
    "\n",
    "    return normalized_embeddings\n",
    "\n",
    "\n",
    "# Normalizing embeddings tensors\n",
    "normalized_embeddings = min_max_normalization(embeddings_tensors)\n",
    "normalized_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with loop check if the embeddings are normalized and padded with same size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, embedding in enumerate(normalized_embeddings[:10]):\n",
    "    print(f\"Tensor {i + 1} shape: {embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should normalize Scores too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalization_scores(scores):\n",
    "    # Convert scores to a tensor\n",
    "    scores_tensor = torch.tensor(scores, dtype=torch.float32)\n",
    "\n",
    "    # Find the minimum and maximum values in the scores\n",
    "    min_score = torch.min(scores_tensor)\n",
    "\n",
    "    max_score = torch.max(scores_tensor)\n",
    "\n",
    "    # Scale the scores to the range [0, 1]\n",
    "    normalized_scores = (scores_tensor - min_score) / (max_score - min_score)\n",
    "\n",
    "    return normalized_scores.tolist()  # Convert tensor back to list\n",
    "\n",
    "\n",
    "# Assuming scores is a list of scores between 0 and 100\n",
    "normalized_scores = min_max_normalization_scores(scores)\n",
    "scores_before_romalization = scores\n",
    "normalized_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create dataset with scores and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# Convert normalized scores to tensor\n",
    "normalized_scores_tensor = torch.tensor(normalized_scores, dtype=torch.float32)\n",
    "\n",
    "# Convert normalized embeddings to tensor\n",
    "embeddings_tensors = [torch.tensor(embedding) for embedding in normalized_embeddings]\n",
    "\n",
    "# Determine the maximum length of embeddings\n",
    "max_length = max(embedding.shape[0] for embedding in embeddings_tensors)\n",
    "\n",
    "# Pad embeddings to match the maximum length\n",
    "padded_embeddings = [torch.nn.functional.pad(embedding, (0, 0, 0, max_length - embedding.shape[0])) for embedding in embeddings_tensors]\n",
    "\n",
    "# Convert padded embeddings to tensor\n",
    "padded_embeddings_tensor = torch.stack(padded_embeddings)\n",
    "\n",
    "# Define a custom dataset class\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, embeddings, scores):\n",
    "        self.embeddings = embeddings\n",
    "        self.scores = scores\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.embeddings[idx], self.scores[idx]\n",
    "\n",
    "# Create a dataset instance\n",
    "dataset = AudioDataset(embeddings_tensors, normalized_scores_tensor)\n",
    "\n",
    "# Create a DataLoader for batching and shuffling\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Example usage of DataLoader\n",
    "for batch in dataloader:\n",
    "    embeddings_batch, scores_batch = batch\n",
    "    print(\"Embeddings Batch Shape:\", embeddings_batch.shape)\n",
    "    print(\"Scores Batch:\", scores_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make LSTM model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AudioModel(nn.Module):\n",
    "    def __init__(self, input_size=37, hidden_size=128, num_layers=1, dropout_prob=0.0, bidirectional=True, weight_decay=0.0):\n",
    "        super(AudioModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        self.dropout = nn.Dropout(dropout_prob)  # Dropout layer\n",
    "        self.fc = nn.Linear(hidden_size * 2 if bidirectional else hidden_size, 1)  # Output score\n",
    "        self.weight_decay = weight_decay  # L2 regularization strength\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, seq_length, input_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        # lstm_out shape: (batch_size, seq_length, hidden_size * num_directions)\n",
    "        # Only take the last hidden state\n",
    "        lstm_out = self.dropout(lstm_out)  # Apply dropout\n",
    "        # Apply fully connected layer\n",
    "        output = self.fc(lstm_out[:, -1, :])\n",
    "        output = torch.sigmoid(output) * 100\n",
    "        return output\n",
    "\n",
    "    def l2_regularization_loss(self):\n",
    "        l2_reg_loss = 0.0\n",
    "        for param in self.parameters():\n",
    "            l2_reg_loss += torch.norm(param, p=2)**2\n",
    "        return self.weight_decay * l2_reg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now We Train model\n",
    "For GPU training Just need to run cell bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(0.8 * dataset_size)  # 80% for training, 20% for testing\n",
    "test_size = dataset_size - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Create DataLoader for training and testing sets\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)  # No need to shuffle test data\n",
    "\n",
    "# Define hyperparameters\n",
    "input_size = 37  # Dimensionality of input embeddings\n",
    "hidden_size = 256  # Size of LSTM hidden states 32-512\n",
    "num_layers = 10   # Number of LSTM layers\n",
    "learning_rate = 0.001\n",
    "num_epochs = 70\n",
    "dropout_prob = 0.0\n",
    "weight_decay = 0.0 # L2 regularization strength\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = AudioModel(input_size, hidden_size, num_layers, dropout_prob=dropout_prob)\n",
    "if torch.cuda.is_available():\n",
    "  model.to(device) \n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)  # Add L2 regularization\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for embeddings, scores in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        if torch.cuda.is_available():\n",
    "          embeddings = embeddings.to(device)\n",
    "          scores = scores.to(device)\n",
    "        # Forward pass\n",
    "        outputs = model(embeddings)\n",
    "        scores = scores.expand_as(outputs)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(outputs, scores)  # Assuming scores are floats\n",
    "        \n",
    "        # Add L2 regularization loss\n",
    "        loss += model.l2_regularization_loss()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss for the epoch\n",
    "    average_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {average_loss:.4f}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for embeddings, scores in test_dataloader:\n",
    "        if torch.cuda.is_available():\n",
    "          embeddings = embeddings.to(device)\n",
    "          scores = scores.to(device)\n",
    "        outputs = model(embeddings)\n",
    "        #outputs = torch.clamp(outputs, min=0, max=100)\n",
    "        test_loss += criterion(outputs.squeeze(), scores.float()).item()\n",
    "\n",
    "test_loss /= len(test_dataloader)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# After training, you can save the model if needed\n",
    "#torch.save(model.state_dict(), 'audio_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for embeddings, scores in test_dataloader:\n",
    "        if torch.cuda.is_available():\n",
    "          embeddings = embeddings.to(device)\n",
    "          scores = scores.to(device)\n",
    "        outputs = model(embeddings)\n",
    "        print(outputs)\n",
    "        predictions.append(outputs.item())  # Append the scalar value\n",
    "\n",
    "        # Assuming scores is also a scalar value, you can directly append it to targets\n",
    "        print(scores)\n",
    "        targets.append(scores.item())\n",
    "\n",
    "\n",
    "# Calculate accuracy\n",
    "def calculate_accuracy(predictions, targets, tolerance=0.09):\n",
    "    correct_predictions = 0\n",
    "    total_predictions = len(predictions)\n",
    "\n",
    "    for pred, target in zip(predictions, targets):\n",
    "        if abs(pred - target) <= tolerance:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "accuracy = calculate_accuracy(predictions, targets)\n",
    "print(\"Accuracy:\", accuracy * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
